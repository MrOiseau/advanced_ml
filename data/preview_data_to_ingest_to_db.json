[
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nQuery Rewriting for Retrieval-Augmented Large Language Models\nXinbei Ma1,2,∗ , Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\nAbstract\nLarge Language Models (LLMs) play pow-\nerful, black-box readers in the retrieve-then-\nread pipeline, making remarkable progress\nin knowledge-intensive tasks. This work in-\ntroduces a new framework, Rewrite-Retrieve-\nRead instead of the previous retrieve-then-read\nfor the retrieval-augmented LLMs from the per-\nspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\nor the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve con-\ntexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is con-\nducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, in-\ndicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM 1. 1 Introduction\nLarge Language Models (LLMs) have shown re-\nmarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings. 1https://github.com/xbmxb/RAG-query-rewriting\nof the real world. This affects the reliability of LLMs\nand hinders wider practical application, because\nthe consistency between the LLM responses with\nthe real world needs further validation. In fact, retrieval-\naugmented LLMs have been shown so effective\nthat they have been regarded as a standard solu-\ntion to alleviate the factuality drawbacks in naive\nLLM generations. Retrieval augmentation is ap-\nplied to select relative passages as external contexts\nfor the language model, which isretrieve-then-read\nframework (Lewis et al., 2020b; Karpukhin et al.,\n2020; Izacard et al., 2022). Take the open-domain\nQuestion-Answering task (open-domain QA) as\nan example, a retriever first searches for related\ndocuments for a question. Then the LLM receives\nthe question and the documents, then predicts an\nanswer. As most LLMs are only accessible through infer-\nence APIs, they play the part of black-box frozen\nreaders in the pipeline. This makes previous re-\ntrieval augmentation methods that require complete\naccess (Lewis et al., 2020b; Guu et al., 2020; Izac-\nard et al., 2022) no longer feasible. Recent studies\non retrieval-augmented language models lean more\non the LLM-oriented adaptation. An idea is to train\na dense retrieval model to cater to the frozen lan-\nguage model (Shi et al., 2023). By using feedback\nfrom the LLM as a training objective, the retrieval\nmodel is tuned for better LLM input contexts. An-\nother research line focuses on the design of inter-\nactions between the retriever and the reader (Yao\net al., 2023; Khattab et al., 2022), where both the\narXiv:2305.14283v3  [cs.CL]  23 Oct 2023"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(Ouyang et al., 2022;\nBrown et al., 2020; Chowdhery et al., 2022). How-\never, the training process depends on large-scale\nhigh-quality corpora but without the perception\n∗ Work done during an internship at 3Microsoft Research\nAsia. # Equal contribution. †Corresponding author. This paper was partially supported by Joint Research\nProject of Yangtze River Delta Science and Technology Inno-\nvation Community (No. 2022CSJGG1400). Thus, LLMs still have to face\nthe issue of hallucination (Yao et al., 2023; Bang\net al., 2023) and temporal misalignment (Röttger\nand Pierrehumbert, 2021; Luu et al., 2022; Jang\net al., 2022). Exist-\ning work has proved that incorporating external\nknowledge (i.e., non-parametric knowledge) with\ninternal knowledge (i.e., parametric knowledge)\ncan effectively alleviate hallucination, especially\nfor knowledge-intensive tasks."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nretriever and the reader are usually frozen. The black-box retriever and the reader form a\nfrozen system. The neural retriever and reader are both PrLMs\nof trainable size like BERT (Devlin et al., 2019)\nor BART (Lewis et al., 2020a)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe idea\nis to trigger the emergent ability through carefully\ncrafted prompts or a sophisticated prompt pipeline."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nMultiple interactions with external knowledge al-\nlow the LLM to approach the correct answer step\nby step. However, there are still problems remaining to\nbe solved. Our proposed methods are evaluated on\nknowledge-intensive downstream tasks including\nopen-domain QA (HotpoQA (Yang et al., 2018),\nAmbigNQ (Min et al., 2020), PopQA (Mallen\net al., 2022)) and multiple choice QA (MMLU\n(Hendrycks et al., 2021))."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nExisting approaches overlook the adap-\ntation of the query, i.e., the input of the retrieve-\nthen-read pipeline."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe retrieval query is either\noriginal from datasets or directly determined by the\nblack-box generation, thus is always fixed. This limits performance and places a burden\non retrieval capability enhancement and prompt\nengineering. We introduce\na tuneable scheme with a small, trainable model,\nachieving performance gains with less resource\nconsumption. More recently, retrieval remains a powerful\nenhancement as the size of models and data scales\nrapidly (Mallen et al., 2022; Shi et al., 2023; Brown\net al., 2020). On the other hand, retrieval enhance-\nment can compensate for the shortfall in parameter\nsize, compared to large-scale language models. For\nexample, by jointly training the retriever and the\nreader, Atlas (Izacard et al., 2022) shows few-shot\nperformance on par with 540B PalM (Chowdhery\net al., 2022) but be of 50× smaller size."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nHow-\never, there is inevitably a gap between the input\ntext and the knowledge that is really needed to\nquery. Thus factual information like\ncommon sense or real-time news helps with output\nprediction through contextualized reading compre-\nhension."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nIn consideration of this issue, this paper pro-\nposes Rewrite-Retrieve-Read, a new framework for\nretrieval augmentation, which can be further tuned\nfor adapting to LLMs. In front of the retriever, a\nstep of rewriting the input is added, filling the gap\nbetween the given input and retrieval need, as is\nshown in Figure 1. To sum up, our proposed novel retrieval-\naugmentation method, rewrite-retrieve-read is the\nfirst framework where the input text is adapted for\nthe frozen retriever and LLM reader. 2 Related Work\n2.1 Retrieval Augmentation\nLanguage models require external knowledge to al-\nleviate the factuality drawbacks. Retrieval augmen-\ntation has been regarded as the standard effective\nsolution. With a retrieval module, related passages\nare provided to the language model as the context\nof the original input. Earlier studies use sparse retriever (Chen et al.,\n2017) or dense retriever (Karpukhin et al., 2020)\nin front of a pre-trained language model (PrLM). Hence, the whole\nretrieve-then-reader framework is a tuneable end-\nto-end system, where the retrieved contexts can\nbe regarded as the intermediate results (Karpukhin\net al., 2020; Lewis et al., 2020b). Approaches to\nsmooth the two-step framework are proposed to op-\ntimize the retrieval and the reading comprehension\n(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,\n2022)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWe adopt the off-the-shelf tool,\nan internet search engine, as the retriever, which\navoids the maintenance of the search index and\ncan access up-to-date knowledge (Lazaridou et al.,\n2022). The Internet as a knowledge baseMore related\nto our work, the search engine can assume the role\nof the retriever and use the Internet as the source of"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nDifferent from previous studies (Khattab\net al., 2022; Yao et al., 2023) that require the mem-\nory of multiple interaction rounds between the re-\ntriever and the LLM for each sample, the motiva-\ntion of our rewriting step is to clarify the retrieval\nneed from the input text. To further smooth the steps of\nour pipeline, we apply a small, trainable language\nmodel to perform the rewriting step, denoted as the\nrewriter. The rewriter is trained by reinforcement\nlearning using the LLM performance as a reward,\nlearning to adapt the retrieval query to improve the\nreader on downstream tasks. The experiments are\nimplemented on T5-large (Raffel et al., 2020) as\nthe rewriter, ChatGPT (Ouyang et al., 2022) and\nVicuna-13B (Chiang et al., 2023) as the LLM\nreader. The results show that query rewriting con-\nsistently improves the retrieve-augmented LLM\nperformance. The results also indicate that the\nsmaller language model can be competent for query\nrewriting."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWe also propose a trainable scheme for our\nrewrite-retrieve-read framework (Figure 1 (c))."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nInput\nRetriever\nOutput\nDocuments\nInput\nWeb Search\nDocuments\nBlack-box LLM\nQuery\nInput\nDocuments\nQuery\nOutput\n Output\nReward\nInput:\nWhat profession does Nicholas Ray and \nElia Kazan have in common? Query: Nicholas Ray profession\nNicholas Ray American author and \ndirector, original name Raymond \nNicholas Kienzle, born August 7, \n1911, Galesville, Wisconsin, U.S......\ndirector\nRewriter\nRetriever\nBlack-box LLM\nReader\nBlack-box LLM\nReader\n(a) Retrieve-then-read    (b)Rewrite-retrieve-read                  (c) Trainable rewrite-retrieve-read    \nBlack-box LLM\nReader\nWeb Search\nRetriever\nRewriter\nSmall PrLM\nExample\nQuery: Elia Kazan profession\nElia Kazan was an American film and \ntheatre director, producer, \nscreenwriter and actor, described  ...... This\nconstraint means nothing is available except input\nand output texts."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nCorrect (reader      )\nHit (retriever      )\n✅\n✅\nFigure 1: Overview of our proposed pipeline. From left to right, we show (a) standard retrieve-then-read method,\n(b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter. Different from the inspiring work mentioned\nabove, our proposed pipeline contains a query\nrewriting step in front of the retrieve-then-read\nmodule."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nexternal knowledge. For\nlarge-scale models, web search still shows effec-\ntive for knowledge augmentation (Lazaridou et al.,\n2022), fact-checking (Menick et al., 2022), and\nLLM agent enhancement (Yao et al., 2023)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nKomeili et al."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(2022) use an\ninternet search for relevant information based on\nthe dialogue history to perform dialogue response\ngeneration. SeeKeR (Shuster et al., 2022) use a\nsingle Transformer to iteratively perform search\nquery generation, then knowledge extraction for\ndialogue generation and sentence completion."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n2.2 Cooperation with Black-box LLMs\nLarge Language Models, such as ChatGPT\n(Ouyang et al., 2022), Codex (Chen et al., 2021),\nPaLM (Chowdhery et al., 2022), emerge impres-\nsive natural language processing ability as well as\nremarkable scalability. This leads to a tendency\nto embrace LLMs on a wide range of NLP tasks. However, LLMs are only accessible as a black box\nin most cases, which is because (i) Some like Chat-\nGPT are not open-source and kept private; (ii) The\nlarge parameter scale requires computational re-\nsources that are not always affordable to users. Existing studies have proved that LLMs’ abili-\nties can be better leveraged by carefully designed\ninteraction methods. GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can re-\ntrieve internal knowledge by prompting. Despite the promising performance in the zero or\nfew-shot setting, the behavior of LLMs sometimes\nneeds adjustments. With the same purpose,\nDirectional Stimulus Prompting (Li et al., 2023)\ndeploys a small model to provide the LLM with\nstimulus (e.g., keywords for summarization, or di-\nalogue actions for response generation), which is\nupdated according to the LLM reward."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nReAct\n(Yao et al., 2023) and Self-Ask (Press et al., 2022)\ncombines the Chain-of-Thought (CoT) (Wei et al.,\n2022; Wang et al., 2022) and inter-actions with web\nAPIs. Unlike ReAct, DSP integrates\nprompts for demonstration bootstrap besides multi-\nhop breakdown and retrieval."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nOnly relying on prompt construction, Re-\nAct provides novel baselines for interactive tasks."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nDemonstrate–Search–Predict (DSP) (Khattab et al.,\n2022) defines a sophisticated pipeline between an\nLLM and a retriever. A feasible approach is to ap-\npend trainable small models in front of or after the\nLLM. The small models, as a part of the parameters\nof the system, can be fine-tuned for optimization. RePlug (Shi et al., 2023) is proposed to fine-tune a\ndense retriever for the frozen LLM in the retrieve-\nthen-read pipeline. The retriever is trained under\nthe LLM’s supervision to retrieve documents that\nare suitable for the LLM. We further propose a trainable scheme\nwith a small rewriting model, which is a novel\nenhancement for retrieval-augmented LLM by re-"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nconstructing the search query. 3 Methodology\nWe present Rewrite-Retrieve-Read, a pipeline that\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. 3.1 Rewrite-Retrieve-Read\nA task with retrieval augmentation can be de-\nnoted as follows. Given a dataset of a knowledge-\nintensive task (e.g., open-domain QA), D =\n{(x, y)i}, i= 0, 1, 2, . (i) Query rewrite: generate a query ˜x\nfor required knowledge based on the original input\nx. (ii) Retrieve: search for related context,doc. A straightforward but effective method is to ask\nan LLM to rewrite queries to search for informa-\ntion that is potentially needed. First, we construct a\npseudo dataset for the query rewriting task. In-\nspired by recent distillation methods (Hsieh et al.,\n2023; Ho et al., 2022), we prompt the LLM to\nrewrite the original questions x in the training set\nand collect the generated queries ˜x as pseudo la-\nbels. Maximization"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nFigure 1 shows an\noverview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\nin section 3.2. , N, x (e.g., a question)\nis the input to the pipeline, y is the expected output\n(e.g., the correct answer). Our pipeline consists of\nthree steps."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n. ."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(iii)\nRead: comprehend the input along with contexts\n[doc, x] and predict the output ˆy. 3.2 Trainable Scheme\nBesides, total reliance on a frozen LLM has shown\nsome drawbacks. To better align to the frozen modules, it is\nfeasible to add a trainable model and adapt it by\ntaking the LLM reader feedback as a reward. The collected samples are then filtered: Those\nthat get correct predictions from the LLM reader\nare selected into the warm-up dataset, denoted as\nDT rain= {(x, ˜x)|ˆy = y}. (i) The state space S is a finite set\nlimited by the vocabulary and the sequence length. (ii) The action space A is equals to the vocabulary. (iv) The reward function R gives a reward\nvalue that depends on the current state. The pol-\nicy gradient is derived from rewards, used as the\ntraining objective. (v) γ denotes the discount fac-\ntor. At each\nstep t, the action at is to generate the next token\nˆ˜xt based on the observation of the present state,\nst = [x, ˆ˜x<t]. When the generation is stopped by\nthe End-Of-Sentence token, one episode is ended. After finishing the retrieval and reading, a reward\nis computed by evaluating the final output, i.e., a\nscore for the LLM reader prediction."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWe use a few-shot\nprompt to encourage the LLM to think, and the\noutput can be none, one or more queries to search. Reasoning errors or invalid\nsearch hinders the performance (Yao et al., 2023;\nBehnamGhader et al., 2022). On the other hand,\nretrieved knowledge may sometimes mislead and\ncompromise the language model (Mallen et al.,\n2022). Highly relying\non the human-written prompt line, ˜x can be sub-\noptimal."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nBased on our framework, we further propose to\nutilize a trainable small language model to take\nover the rewriting step, as is shown in the right\npart of Figure 1. The trainable model is initial-\nized with the pre-trained T5-large (770M) (Raffel\net al., 2020), denoted astrainable rewriter, Gθ. The\nrewriter is first trained on pseudo data to warm up\n(§3.2.1), then continually trained by reinforcement\nlearning (§3.2.2). 3.2.1 Rewriter Warm-up\nThe task, query rewriting, is quite different from\nthe pre-training objective of sequence-to-sequence\ngenerative models like T5. The rewriter Gθ is fine-\ntuned on DT rainwith the standard log-likelihood\nas the training objective, denoted as\nLwarm = −\nX\nt\nlogpθ( ˆ˜xt | ˜x<t, x ). (1)\nThe rewriter model after warm-up shows mod-\nest performance, which depends on the pseudo\ndata quality and rewriter capability. The relatively small scale of the rewriter\nsize is also a limitation of the performance after the\nwarm-up. Then we turn to reinforcement learning\nto align the rewriter to the following retriever and\nLLM reader. 3.2.2 Reinforcement Learning\nTo further fine-tune the rewriter to cater to the LLM\nreader, we adopt a policy gradient reinforcement\nlearning framework. Task Formulation In the context of reinforce-\nment learning, the rewriter optimization is for-\nmulated as a Markov Decision Process 5-tuple\n⟨S, A, P, R, γ⟩. (iii) The transition probability P is determined by\nthe policy network, which is the rewriter model\nGθ. More specifically, the rewriter Gθ after the\nwarm-up is the initial policy model π0. Policy Optimization We adopt Proximal Policy\nOptimization (PPO) (Schulman et al., 2017), fol-\nlowing (Ramamurthy et al., 2022)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nof the expectation of the reward R is formulated as\nmax\nθ\nEˆ˜x∼pθ(·|x)[R(x, ˆ˜x)],\nmax\nθ\nE(st,at)∼πθ′ [min{kt,θAθ′\n(st, at) ;\nclip (kt,θ, 1 − ε, 1 +ε) Aθ′\n(st, at)}],\nkt,θ = pθ (at | st)\npθ′ (at | st),\n(2)\nwhere θ′ is the temporarily fixed policy for sam-\npling and θ is updated. A denotes the advantage\nfunction, which is formulated based on the estima-\ntion of value network Vϕ. The value network Vϕ is\ninitialized from the policy network π0. The formu-\nlation follows Generalized Advantage Estimation\n(GAE) (Schulman et al., 2015). δt = R (st, at) +Vϕ (st+1) − Vϕ (st) ,\nˆAθ\nt (st, at) =\n∞X\nt′=0\nλt′\nδt+t′ , (3)\nwhere λ is the bias-variance trade-off parameter. The reward function R reflects the quality of the\ngenerated queries, which needs to be consistent\nwith the final evaluation of the task. A\npart of the reward function is the measures of ˆy\ncompared to the golden label y (e.g., exact match\nand F1 of the predicted answers), denoted as Rlm. R (st, at) =Rlm(ˆ˜x, y) − βKL (πθ∥π0) . (4)\nThe final loss function is composed of policy loss\nand value loss. Lθ = − 1\n|S| T\nX\nτ∈S\nTX\nt=0\nmin(kt,θAθ′\n, clip Aθ′\n),\nLϕ = 1\n|S| T\nX\nτ∈S\nTX\nt=0\n(Vϕ (st) − Rt)2 ,\nLppo = Lθ + λvLϕ. (5)\nHere, S denotes the sampled set, and T is for step\nnumbers. We evaluate the full test set."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nˆ˜x is fed to the\nretriever and the reader for a final prediction ˆy. But it allows for a wide knowledge scope\nand up-to-time factuality. Reader The reader is a frozen LLM, where we\nadopt ChatGPT (gpt-3.5-turbo) and Vicuna-13B. It performs reading comprehension and prediction\nwith few-shot in-context learning. In our prompt,\nfollowing the brief instruction and the demonstra-\ntions, the input is x or [doc, ˆ˜x] with retrieval aug-\nmentation. It has been proved that both the phrasing of\nprompt lines (Zhang et al., 2023a) and the selection\nof demonstrations show effects on the in-context\nlearning performance (Su et al., 2022; Zhang et al.,\n2023b). As it is not the focus of this work, we pay\nno more attention to prompt editing. 5 Experiments\n5.1 Task Settings\n5.1.1 Open-domain QA\nThree open-domain QA datasets are used for evalu-\nation."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nBesides, a KL-divergence regularization is added\nto prevent the model from deviating too far from\nthe initialization (Ramamurthy et al., 2022; Ziegler\net al., 2019). 4 Implementation\nRewriter For the frozen pipeline in §3.1, we\nprompt an LLM to rewrite the query with few-shot\nin-context learning (Brown et al., 2020; Min et al.,\n2022). Our prompt follows the formulation of [in-\nstruction, demonstrations, input], where the input\nis x. The instruction is straightforward and demon-\nstrations are 1-3 random examples from training\nsets and are kept constant across all runs, mainly\nfor the task-specific output format illustration, i.e.,\na short phrase as an answer for HotpotQA, and an\noption as an answer for MMLU. For the training\nscheme in §3.2, we fine-tuning a T5 as the rewriter. It requires no candidate index construc-\ntion like a dense retriever, nor candidates like a\ntextbook. Then we use\nBM25 to keep those with higher relevance scores\nwith the query, reducing the document length."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nRetriever We use the Bing search engine as the\nretriever. With Bing API, the re-\ntrieval is performed in two approaches. (i) For all\nretrieved web pages, we concatenate the snippets\nthat are related sentences selected by Bing. This\nmethod is similar to using a search engine in a\nbrowser, input a query and press Enter, then col-\nlect the texts shown on the search result page. (ii)\nFor retrieved web pages, we request the URLs and\nparser to get all the texts. This is similar to clicking\non items on the search result page."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(i) HotPotQA (Yang et al., 2018) consists of\ncomplex questions that require multi-hop reason-\ning. (ii) AmbigNQ\n(Min et al., 2020) provides a disambiguated version\nof Natural Questions (NQ) (Kwiatkowski et al.,\n2019). For ambiguous questions in NQ, minimal\nconstraints are added to break it into several similar"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nDirect prompt\nAnswer the question in the following format, end the answer with ’**’. {demonstration} Question: { x} Answer:\nReader prompt in retrieval-augment pipelines\nAnswer the question in the following format, end the answer with ’**’. Split the queries with ’;’ and end the queries with ’**’. {demonstration} Question: { x} Answer:\nMultiple choice QA: Provide a better search query for web search engine to answer the given question, end the\nqueries with ’**’. {demonstration} Question: { x} Answer:\nTable 1: Prompt lines used for the LLMs. The answer is one option. Table 1 presents prompt line forms."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n{demonstration} Question: { doc } { x}\nAnswer:\nPrompts for LLM as a frozen rewriter\nOpen-domain QA: Think step by step to answer this question, and provide search engine queries for knowledge\nthat you need. We use\nChatGPT for both the reader and the frozen rewriter. (iii) LLM\nas a frozen rewriter: As is introduced in §3.1,\nwe prompt a frozen LLM to reason and generate\nqueries by few-shot in-context learning. (iv) Train-\nable rewriter: Applying the fine-tuned rewriter,\nthe output queries are used by the retriever and the\nreader. For the three datasets, query\nrewriting consistently brings performance gain\nwith both a frozen rewriter and a trainable rewriter. This suggests\nthat multi-hop questions are not suitable queries\nfor the web search engine. On PopQA, our trainable\nrewriter surpasses standard retrieval while being\ninferior to the LLM rewriter."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nbut specific questions. (6)\n5.1.2 Multiple-choice QA\nFor multiple-choice QA, our evaluation is con-\nducted on Massive Multi-task Language Under-\nstanding (MMLU) (Hendrycks et al., 2021), an\nexam question dataset including 4 categories: Hu-\nmanities, STEM, Social Sciences, and Other. (i) Direct: The\nstandard in-context learning without any augmen-\ntations. Please\nnote that the prompts for prediction are kept the\nsame for each task."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe first 1000 samples are\nevaluated in the test set. We split the dataset into 13k for training\nand 714 for testing. Each\ncategory is split into 80% for the training set and\n20% for the test set."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(iii) PopQA (Mallen et al.,\n2022) includes long-tail distributions as it contains\nmore low-popularity knowledge than other popular\nQA tasks. Open-domain QA benchmarks are sets of\nquestion-answer pairs denoted as {(q, a)i}. Multiple-choice QA can be formulated as\n{(q′, a)i}, where q′ = [q, c0, c1, c2, c3]. c denotes\nthe options, generally there are four for each ques-\ntion. 5.3 Results\nExperimental results on open-domain QA are re-\nported in Table 2. This shows that\nusing complex questions as queries cannot com-\npensate for the parametric knowledge, but bring\nnoises instead (Mallen et al., 2022). This indicates that the"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe evaluation metrics are Exact Match (EM ) and\nF1 scores. For the reward function in RL, we use\nan indicator to reward if the retrieved content hits\nthe answer and penalize if misses the answer, de-\nnoted as Hit. The total reward is a weighted sum\nof EM, F1, and Hit. Hit =\n(\n1 a in doc,\n−1 else\nRlm = EM + λf F1 + λhHit. EM\nis reported as metrics and used for the reward. Rlm = EM. The scores increase by\nadding the rewriting step."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe retrieved documents that are included\nin the officially provided contaminated lists are\nignored."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe questions with options are rewritten\ninto search queries. More\ninformation on datasets and training setup are pre-\nsented in the appendix. (ii) Retrieve-then-read: The standard\nretrieval-augmented method. Retrieved documents\nare concatenated with the question. On AmbigNQ and PopQA, the standard retrieval\naugments the reader, indicating useful external\nknowledge is retrieved. On HotpotQA, the stan-\ndard retrieval hurts the reader."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(7)\nWe use ChatGPT as a frozen rewriter and the reader. We also use Vicuna-13B as the reader for evalua-\ntion due to the rate limit issue of ChatGPT. 5.2 Baselines\nThe following settings are implemented to eval-\nuate and support our methods."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\ndistillation of query rewriting is sub-optimal. Model EM F 1\nHotpotQA\nDirect 32.36 43.05\nRetrieve-then-read 30.47 41.34\nLLM rewriter 32.80 43.85\nTrainable rewriter 34.38 45.97\nAmbigNQ\nDirect 42.10 53.05\nRetrieve-then-read 45.80 58.50\nLLM rewriter 46.40 58.74\nTrainable rewriter 47.80 60.71\nPopQA\nDirect 41.94 44.61\nRetrieve-then-read 43.20 47.53\nLLM rewriter 46.00 49.74\nTrainable rewriter 45.72 49.51\nTable 2: Metrics of open-domain QA. 0 5 10 15 20 25\nInteration\n30\n31\n32\n33\n34EM\n(a)HotpotQA\nRetrieve-then-read\nLLM rewriter\n41\n42\n43\n44\n45\nF1\n0 2 4 6 8 10\nInteration\n44\n45\n46\n47\n48EM\n(b)AmbigNQ\nRetrieve-then-read\nLLM rewriter\n57\n58\n59\n60\nF1\n0 2 4 6 8 10 12\nInteration\n40\n41\n42\n43\n44\n45\n46EM\n(c)PopQA\nRetrieve-then-read\nLLM rewriter 43\n44\n45\n46\n47\n48\n49\nF1\nFigure 2: Reinforcement learning validation scores of\n(a)HotpotQA, (b)AmbigNQ, and (c)PopQA. (iii)\nIn particular, on PopQA, the trainable rewriter re-\nmains inferior to the LLM rewriter. LLM rewriter knows better\nwhen the retrieval is needed for itself as a reader,\nalthough the rewriting step is not concatenated as"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe scores on multiple-choice QA are presented\nin Table 3. This section shows\nthe validation scores of the three open-domain QA\ndatasets for further analysis. Figure 2 presents\nthe metric scores through training iterations in the\nprocess of reinforcement learning."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWith ChatGPT as a reader, it can be ob-\nserved that query rewriting improves the scores in\nmost of the settings, except for the social sciences\ncategory. With Vicuna as a reader, our method\nachieves more gains on the four categories com-\npared to ChatGPT. STEM Other Social\nChatGPT\nDirect 75.6 58.8 69.0 71.6\nRetrieve-then-read 76.7 63.3 70.0 78.2\nLLM rewriter 77.0 63.5 72.6 76.4\nVicuna-13B\nDirect 39.8 34.9 50.2 46.6\nRetrieve-then-read 40.2 39.8 55.2 50.6\nLLM rewriter 42.0 41.5 57.1 52.2\nTrainable rewriter 43.2 40.9 59.3 51.2\nTable 3: Metrics of multiple choice QA."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThis agrees with the intuition\nthat a more powerful reader has more parametric\nmemories, thus more difficult to compensate with\nexternal knowledge. This causes more complex-\nity and uncertainty."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nMMLU EM\nHuman."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n6 Analysis\n6.1 Training Process\nThe training process includes two stages, warm-up\nand reinforcement learning. As the rewriting\nmodels have been warmed up on the pseudo data\nbefore RL, scores at “0 iteration” denote the ability\nacquired from the warm-up training. This indicates that the RL training stage can\ncompensate for the insufficiency of the distillation\non the pseudo data during warm-up training."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe solid\nlines show EM (red) and F1 (blue) numbers through\ntraining iterations. The dashed lines are EM scores\nof the standard retrieve-then-read method (orange) and\nretrieval with an LLM as the rewriter (green). It can be observed that the curves show upward\ntrends with some fluctuations on all the datasets."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n(i)\nFor multi-hop questions in HotpotQA, the standard\nretrieval is relatively weaker. Complex questions\ncan be not specific search queries and show a larger\ngap from rewritten queries, i.e., the green and red\nlines. (ii) On AmbigNQ and PopQA, our method\nsurpasses the baselines after several iterations (3\nor 4). This can be\nexplained as the dataset is constructed for adaptive\nretrieval (Mallen et al., 2022), which only uses re-\ntrieval where it helps to avoid harmful redundant\nretrieval. Thus, “None” is a possible query that\nmeans no retrieval."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nthe input context of the reader. Hence, QA metrics are indirect measurements. We\ntake a closer look at the retrieved context and the\nreader capability through the retrieval metric, hit\nratio. After text normalization, the hit rate is com-\nputed to measure whether the retrieved context con-\ntains the correct answers. The\nscores in the second line are computed on a selec-\ntion of the samples whose retrieved contexts hit\ncorrect answers (under the standard retrieve-then-\nread setting). The scores show the approximate\nupper bound ability of the reader with retrieval aug-\nmentation, abbreviated as the “upper bound” score. The effectiveness of retrieval is proved compared\nto the no retrieval setting (the first line). ✅ ✅\n✅ ✅\n✅ ✅\n✅ ✅\n❌ ❌\n❌ ❌\n❌ ❌\n❌ ❌\nFigure 3: Examples for intuitive illustration. Hit means retriever recall\nthe answer, while Correct is for the reader output. We also observed that the improvement in\nthe hit rate of the retriever is more significant than\nthe improvement in the reader."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWe calculate the performance of query “None”. The questions that can be correctly answered with-\nout retrieval (i.e., the “Direct” method) are those\nsamples that need no more context. Comparing this\nretrieval-free set with those that are rewritten to\nbe“None” query, the F1 score of the LLM rewriter\nis 71.9% and the T5 rewriter score is 67.1%. If\nwe consider the questions that can be correctly an-\nswered without retrieval but go wrong with retrieval\nas the retrieval-free set, the F1 scores are 78.7% for\nLLM rewriter and 77.4% for T5. Model EM F 1 Hit ratio\nNo retrieval 42.10 53.05 –\nUpper bound 58.40 69.45 100\nRetrieve-then-read\nw/ snippet 38.70 50.50 61.1\nw/ BM25 45.80 58.50 76.4\nLLM rewriter\nw/ snippet 39.80 52.64 63.5\nw/ BM25 46.40 58.74 77.5\nTrainable rewriter\nw/ BM252 47.80 60.71 82.2\nTable 4: Retrieval analysis on AmbigNQ. 6.2 Retrieval Result\nOur proposed method is a pipeline framework, in-\nstead of an end-to-end system. The query rewrit-\ning first affects the retrieved context, then the con-\ntext makes a difference to the output of the reader. Table 4 shows the scores on AmbigNQ. For each\nretrieval method, two settings are presented: (i)\ncollecting Bing snippets, (ii) selecting from URLs\nby BM25. The metrics show that content selection\nwith BM25 recalls better documents than snippets,\n2Our trainable rewriter is adapted to the retriever using\nBM25 during RL training. Using the output queries of the test\nset after training, the snippet hit rate is 73.4%. Q0 denotes\noriginal input, Q1 is from the LLM rewriter, and Q2 is\nfrom the trained T5 rewriter. while query rewriting makes progress on both set-\ntings. This is consistent\nwith the findings in related search (Mallen et al.,\n2022; Liu et al., 2023). 6.3 Case Study\nTo intuitively show how the query rewriting makes\na difference in the retrieved contexts and prediction\nperformance, we present examples in Figure 3 to\ncompare the original questions and the queries. The second is an example\nwhere the query from the LLM rewriter failed but"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nExample 1: multi-hop question\nQ0: The youngest daughter of Lady Mary-Gaye \n       Curzon stars with Douglas Smith and \n       Lucien Laviscount in what 2017 film? Q1: the youngest daughter of Lady Mary-Gaye\n       Curzon; 2017 film stars Douglas Smith \n       and Lucien Laviscount\nQ2: Lady Mary-Gaye Curzon youngest daughter\n       2017 film with Douglas Smith and Lucien \n       Laviscount\nExample 2:\nQ1: movie \"All Star\" 2000\nExample 3: multiple choice\nHit Correct\nQ0: A car-manufacturing factory is considering \n       a new site for its next plant. Options: A. Q2: 2000 movie \"All Star\" song\nQ0: What 2000 movie does the song \"All Star\"\n       appear in? In\nexample 1, the original question asks for a film that\nthe youngest daughter of Lady Mary-Gaye Curzon\nco-stars with two certain actors. Both query 1 and\nquery 2 put the keyword film forward, closely fol-\nlowing the youngest daughter of Lady Mary-Gaye\nCurzon. With both, the actress Charlotte Calthorpe\nand her movie information can be retrieved and\nthe answer is included."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWhich of the \n       following would community planners be \n       most concerned with before allowing the \n       plant to be built? The amount \n       of materials stored in the plant B. The hours\n       of operations of the new plant C. The effect \n       the plant will have on the environment D. \n       The work environment for the employees\n       at the plant\nQ1: What would community planners be most \n       concerned  with before allowing a car-\n       manufacturing factory to be built?"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nthe query from T5 gets the correct answer. The\nnumber 2000 is misunderstood in query 1, while\nquery 2 keeps 200 movie together, avoiding mean-\ningless retrieval. 7 Conclusion\nThis paper introduces the Rewrite-Retrieve-Read\npipeline, where a query rewriting step is added\nfor the retrieval-augmented LLM. Evaluation and analyses on open-domain QA\nand multiple-choice QA show the effectiveness\nof query rewriting. Our work proposes a novel\nretrieval-augmented black-box LLM framework,\nproves that the retrieval augmentation can be en-\nhanced from the aspect of query rewriting, and\nprovides a new method for integrating trainable\nmodules into black-box LLMs. (ii) The research line of LLM agent has\nshown impressive performance but relies on mul-\ntiple calls to the LLM for each sample (Khattab\net al., 2022; Yao et al., 2023), where the LLM\nplays as an agent to flexibly call the retriever multi-\nple times, reads the context in earlier hops, and\ngenerates follow-up questions. Different from\nthese studies, our motivation is to enhance the one-\nturn retriever-then-read framework with a trainable\nquery rewriter."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nExample 3 is for multiple choice. The retrieve con-\ntexts are mainly about Introduction to Community\nPlanning where the answer environment appears\nseveral times. More discussion\nis included in the appendix. Reading Wikipedia to answer open-\ndomain questions."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe query simplifies the background and enhances\nthe keyword community planner. This approach\nis applicable for adopting a frozen large language\nmodel as the reader and a real-time web search\nengine as the retriever. Further, we propose to ap-\nply a tuneable small language model the rewriter,\nwhich can be trained to cater to the frozen retriever\nand reader. (iii) Using a web search engine as\nthe retriever also leads to some limitations. Neu-\nral dense retrievers that are based on professional,\nfiltered knowledge bases may potentially achieve\nbetter and controllable retrieval. Can retriever-augmented language\nmodels reason? Advances in neural information processing\nsystems, 33:1877–1901. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe training implementation consists\nof two stages, warm-up and reinforcement learn-\ning. Limitations\nWe acknowledge the limitations of this work. (i)\nThere is still a trade-off between generalization and\nspecialization among downstream tasks. Adding\na training process, the scalability to direct transfer\nis compromised, compared to few-shot in-context\nlearning. BERT: pre-training of"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reason-\ning, hallucination, and interactivity. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n2023. 2022. 2020. 2017. 2021. 2023. 2022. 2019."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\narXiv preprint\narXiv:2302.04023. arXiv preprint\narXiv:2212.09146. CoRR,\nabs/2107.03374. arXiv preprint\narXiv:2204.02311."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. Danqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nthe blame game between the re-\ntriever and the language model. Language models are few-shot\nlearners. In Association for Computational\nLinguistics (ACL). Evaluat-\ning large language models trained on code. Palm: Scaling\nlanguage modeling with pathways."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR. Measuring massive multitask language\nunderstanding. Large language models are reasoning teachers. outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. Few-shot Learning with Retrieval Aug-\nmented Language Models. Retrieval as attention: End-to-end learning of\nretrieval and reading within a single transformer. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. Internet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics. Transactions of the\nAssociation for Computational Linguistics. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering. You only need one model for open-domain\nquestion answering. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3047–3060, Abu Dhabi, United\nArab Emirates. BART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880. Association for Computational Linguistics. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Guiding large\nlanguage models via directional stimulus prompting. Time\nwaits for no one! In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5944–5958, Seattle,\nUnited States."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. Dan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. Namgyu Ho, Laura Schmid, and Se-Young Yun. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander J. Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Omar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Mojtaba Komeili, Kurt Shuster, and Jason Weston. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. Angeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher Manning, and Kyoung-Gu Woo. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. Zekun Li, Baolin Peng, Pengcheng He, Michel Galley,\nJianfeng Gao, and Xifeng Yan. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A. Smith. analysis and challenges of tem-\nporal misalignment."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n2020. 2021. 2022. 2023. 2022. 2022. 2022. 2020. 2022. 2022. 2019. 2022. 2022. 2020a. 2020b. 2023. 2023. 2022."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nProceedings of the International Con-\nference on Learning Representations (ICLR). Natural questions: a benchmark\nfor question answering research. Association for Computational Lin-\nguistics. Association for Computational Lin-\nguistics."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\narXiv\npreprint arXiv:2212.10071. ArXiv, abs/2305.02301. arXiv preprint\narXiv:2212.14024. arXiv preprint arXiv:2203.05115. arXiv preprint arXiv:2302.11520. arXiv preprint\narXiv:2307.03172."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nDistilling step-by-step! Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), Abu Dhabi, UAE. Advances in Neu-\nral Information Processing Systems, 33:9459–9474. Lost in the middle: How lan-\nguage models use long contexts."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. Jacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Rajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKianté Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi. Paul Röttger and Janet Pierrehumbert. Devendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. John Schulman, Philipp Moritz, Sergey Levine, Michael\nJordan, and Pieter Abbeel. John Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V . Le, Ed H. Chi, and Denny Zhou. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n2022. 2022. 2022. 2020. 2022. 2022. 2023. 2020. 2022. 2021. 2021. 2023. 2015. 2017. 2023. 2023. 2022. 2022. 2022. 2022. 2018."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories. Teaching\nlanguage models to support answers with verified\nquotes. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics. AmbigQA: Answering am-\nbiguous open-domain questions. In EMNLP. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744. Measuring\nand narrowing the compositionality gap in language\nmodels. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67. Is reinforcement learning (not) for natural\nlanguage processing? : Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion. Temporal\nadaptation of BERT and performance on downstream\ndocument classification: Insights from social media. In Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981. Toolformer:\nLanguage models can teach themselves to use tools. High-dimensional\ncontinuous control using generalized advantage esti-\nmation. Proxi-\nmal policy optimization algorithms. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. Replug: Retrieval-\naugmented black-box language models. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 373–393. Association for Computational Lin-\nguistics. Selec-\ntive annotation makes language models better few-\nshot learners. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS. HotpotQA: A dataset for"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\narXiv preprint. arXiv preprint arXiv:2203.11147. arXiv preprint arXiv:2210.03350. ArXiv preprint, abs/2307.16789. arXiv preprint arXiv:2302.04761. arXiv preprint arXiv:1506.02438. arXiv preprint\narXiv:1707.06347. arXiv preprint arXiv:2303.17580. arXiv\npreprint arXiv:2301.12652. arXiv preprint arXiv:2209.01975. CoRR, abs/2203.11171."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\ndiverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics. ReAct: Synergizing reasoning and acting in language\nmodels. Generate\nrather than retrieve: Large language models are\nstrong context generators. Tempera:\nTest-time prompt editing via reinforcement learning. Automatic chain of thought prompt-\ning in large language models. The usage of external tools expands the abil-\nity boundary of language models, compensating\nfor the parametric knowledge, and grounding the\ncapabilities of language models to interact with en-\nvironments (Qin et al., 2023; Schick et al., 2023). Recent studies show a trend to leverage plug-and-\nplay tools like search engines to enhance language\nagents (Lazaridou et al., 2022; Menick et al., 2022;\nShuster et al., 2022; Shen et al., 2023)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E Gonzalez. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\n2023. 2023. 2023a. 2023b. 2019. arXiv\npreprint arXiv:1909.08593. (2022); Ziegler et al."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nIn International Conference on Learning\nRepresentations (ICLR). In International Confer-\nence for Learning Representation (ICLR). In The Eleventh International Conference on Learn-\ning Representations. In The Eleventh In-\nternational Conference on Learning Representations\n(ICLR 2023). We\npresent more discussion on the advantages and dis-\nadvantages here."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nFine-tuning lan-\nguage models from human preferences. For those that have no\nofficial splits (PopQA and MMLU), we randomly\nsplit the full dataset. In detail, PopQA contains 16\ntypes of questions, thus split into 13k for training\nand 714 for testing following stratified sampling. For MMLU, each of the 4 categories is randomly\nsplit into 80% for the training set and 20% for\nthe test set."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nA Warm-up Dataset\nFor the warm-up training of the tuneable rewriter,\nwe construct a pseudo dataset for the query rewrit-\ning task. For benchmarks that provide official train-\ning and test splits (HotpotQA and AmbigNQ), we\nuse the whole training set. Then the training sets of each bench-\nmark are used to derive the pseudo dataset for the\nquery rewriting, i.e., DT rain = {(x, ˜x)|ˆy = y}. We present the statistics of the splits and warm-up\ndataset in Table 5."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nB Setup Details\nFor warm-up, we train the T5-large with 3e-5 learn-\ning rate, {16, 20} batch size, for {6,8,12} epochs. For reinforcement learning, we set the sampling\nTask Training Set Warm-up Test Set\nHotpotQA 90.4k 37.5k 7.4k\nAmbigNQ 19.4k 8.6k 1k\nPopQA 13.0k 6.0k 0.7k\nHumanities 3.8k 1.5k 0.9k\nSTEM 2.4k 0.9k 0.6k\nOther 2.6k 1.3k 0.6k\nSocial Science 2.4k 1.3k 0.6k\nTable 5: Metrics of multiple choice QA. After\nsampling, the policy network is trained for {2,3,4}\nepochs, with learning rate as 2e-6 and batch size\nas {8,16}."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nsteps to 5120, 10 threads, 512 steps for each. λf and λh are 1.0. β in Eq. 4 is dy-\nnamically adapted according to Ramamurthy et al. (2019),\net = clip\n\u0012KL (π∥π0) − KLtarget\nKLtarget\n, −0.2, 0.2\n\u0013\n,\nβt+1 = βt (1 + Kβet) ,\nwhere KLtarget is set to 0.2, K β is set to 0.1. β0\nis initialized to be 0.001. The generation strat-\negy follows the 4-beam search and returns the one\nsequence."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nIn the implementation of the BM25-\nbased retriever, the textboxes from searched URLs\nare parsed from HTML code. We compute BM25\nscores between the paragraph from each textbox\nand the query following the scikit-learn package,\nthen keep those with higher scores until the re-\nserved context reaches a max length. In reinforce-\nment learning, the results of AmbigNQ are with\nthe BM25 method, while others use snippets as\ncontext."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nC Web Search: Tool Use\nOur proposed pipeline integrates an externally built\nweb search engine as the retriever module. Search\nengine APIs are well-developed retrievers, saving\nefforts to build and maintain another retriever, like\na Contriever. Accessible to the whole Internet, the\nweb search retrieves from a wide-range, up-to-date"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nknowledge base. On the other hand, web search APIs are commer-\ncial products requiring subscriptions. Also, the vast\namount of knowledge on the web can be difficult\nto control. The retrieved context from the Internet\ncan be occasionally inconsistent, redundant, and\ntoxic, which hinders the LLM reader. Beyond retrieval augmentation, in a general\nscope, other tools called by LLMs, like code in-\nterpreters, online models, and expert applications,\nare all similar to search engines, without trainable\nparameters to optimize."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2023-10-24T01:01:54+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2023-10-24T01:01:54+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models",
            "trapped": "/False",
            "source": "./data/pdfs/[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models.pdf",
            "total_pages": 13,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "[Oct 2023] Query Rewriting for Retrieval-Augmented Large Language Models\n\nThe temporal misalignment prob-\nlem on a fixed candidate database can be alleviated. There could be a gap be-\ntween the LM and these tools. This paper proposes\nan idea to align them through a trainable small\nmodel."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be ﬁne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeciﬁc architecture modiﬁcations. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speciﬁc architectures that\ninclude the pre-trained representations as addi-\ntional features. In this paper, we improve the ﬁne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder Representations from Transformers."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nBERT is conceptually simple and empirically\npowerful."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\n1 Introduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and ﬁne-tuning. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nThese include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce ﬁne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying ﬁne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nThe ﬁne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speciﬁc parameters, and is trained on the\ndownstream tasks by simply ﬁne-tuning all pre-\ntrained parameters. We argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the ﬁne-tuning approaches."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nThe ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nFor\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a “masked lan-\nguage model” (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nThe\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\narXiv:1810.04805v2  [cs.CL]  24 May 2019"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nword based only on its context. Unlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na “next sentence prediction” task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n• We demonstrate the importance of bidirectional\npre-training for language representations. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeciﬁc architectures. 2.1 Unsupervised Feature-based Approaches\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods. Pre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signiﬁcant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013). These approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014). To train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016). They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations. When integrating contextual word embeddings\nwith existing task-speciﬁc architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. 2.2 Unsupervised Fine-tuning Approaches\nAs with the feature-based approaches, the ﬁrst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text (Col-\nlobert and Weston, 2008). More recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\nﬁne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). At least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\net al., 2018a). Left-to-right language model-"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nUn-\nlike Radford et al. This\nis also in contrast to Peters et al. BERT is the ﬁrst ﬁne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speciﬁc architectures. • BERT advances the state of the art for eleven\nNLP tasks. The code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert. 2 Related Work\nThere is a long history of pre-training general lan-\nguage representations, and we brieﬂy review the\nmost widely-used approaches in this section. ELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. Melamud et al. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels. The advantage of these approaches\nis that few parameters need to be learned from\nscratch."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\nBERT BERT\nE[CLS] E1  E[SEP]... EN E1’ ... EM’\nC\n T1\n T[SEP]...\n TN\n T1’ ...\n TM’\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\nQuestion Paragraph\nStart/End Span\nBERT\nE[CLS] E1  E[SEP]... EN E1’ ... EM’\nC\n T1\n T[SEP]...\n TN\n T1’ ...\n TM’\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\nMasked Sentence A Masked Sentence B\nPre-training Fine-Tuning\nNSP Mask LM Mask LM\nUnlabeled Sentence A and B Pair \nSQuAD\nQuestion Answer Pair\nNERMNLI\nFigure 1: Overall pre-training and ﬁne-tuning procedures for BERT. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers). 3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section. A distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. Model Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M). BERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H = 768and 4096 for the H = 1024. 4We note that in the literature the bidirectional Trans-"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\nApart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. ing and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014). There are two steps in our\nframework: pre-training and ﬁne-tuning. Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. There is mini-\nmal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Bert paper\n\nInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., ⟨Question, Answer ⟩) in one token sequence. A “sequence” refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together. We use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The ﬁrst\ntoken of every sequence is always a special clas-\nsiﬁcation token ( [CLS]). The ﬁnal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classiﬁcation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the ﬁnal hidden\nvector of the special [CLS] token as C ∈RH,\nand the ﬁnal hidden vector for the ith input token\nas Ti ∈RH. For a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. (2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. Task #1: Masked LM Intuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model. Unfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly “see itself”, and the model could trivially\npredict the target word in a multi-layered context. former is often referred to as a “Transformer encoder” while\nthe left-context-only version is referred to as a “Transformer\ndecoder” since it can be used for text generation. In order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a “masked\nLM” (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the ﬁnal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input. Although this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\nﬁne-tuning, since the [MASK] token does not ap-\npear during ﬁne-tuning. To mitigate this, we do\nnot always replace “masked” words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTi will be used to predict the original token with\ncross entropy loss. Task #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speciﬁcally,\nwhen choosing the sentencesA and B for each pre-\ntraining example, 50% of the time B is the actual\nnext sentence that follows A (labeled as IsNext),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext). As we show\nin Figure 1, C is used for next sentence predic-\ntion (NSP). 5 Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very beneﬁcial to both QA and NLI. 6\n5The ﬁnal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation\nwithout ﬁne-tuning, since it was trained with NSP."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Bert paper\n\nThroughout this work, a “sentence” can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A visualiza-\ntion of this construction can be seen in Figure 2. 3.1 Pre-training BERT\nUnlike Peters et al. (2018a) and Radford et al. This step\nis presented in the left part of Figure 1. We compare variations of this\nprocedure in Appendix C.2."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\n[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input\nE[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]\nToken\nEmbeddings\nEA EB EB EB EB EBEA EA EA EA EASegment\nEmbeddings\nE0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position\nEmbeddings\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classiﬁcation, such as en-\ntailment or sentiment analysis. To ﬁne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the ﬁnal hid-\nden vector C ∈ RH corresponding to the ﬁrst\ninput token ([CLS]) as the aggregate representa-\ntion."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. 7 We de-\nscribe the task-speciﬁc details in the correspond-\ning subsections of Section 4. More details can be\nfound in Appendix A.5."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\n(2017) and\nLogeswaran and Lee (2018). (2016); Seo et al. (2017)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nHowever, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters. 3.2 Fine-tuning BERT\nFine-tuning is straightforward since the self-\nattention mechanism in the Transformer al-\nlows BERT to model many downstream tasks—\nwhether they involve single text or text pairs—by\nswapping out the appropriate inputs and outputs. BERT\ninstead uses the self-attention mechanism to unify\nthese two stages, as encoding a concatenated text\npair with self-attention effectively includes bidi-\nrectional cross attention between two sentences. For each task, we simply plug in the task-\nspeciﬁc inputs and outputs into BERT and ﬁne-\ntune all the parameters end-to-end. 4 Experiments\nIn this section, we present BERT ﬁne-tuning re-\nsults on 11 NLP tasks. 4.1 GLUE\nThe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. We com-\npute a standard classiﬁcation loss with C and W,\ni.e., log(softmax(CWT ))."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nPre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages\nand ignore lists, tables, and headers. It is criti-\ncal to use a document-level corpus rather than a\nshufﬂed sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to\nextract long contiguous sequences."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nFor applications involving text pairs, a common\npattern is to independently encode text pairs be-\nfore applying bidirectional cross attention, such\nas Parikh et al. At the in-\nput, sentence A and sentence B from pre-training\nare analogous to (1) sentence pairs in paraphras-\ning, (2) hypothesis-premise pairs in entailment, (3)\nquestion-passage pairs in question answering, and\n(4) a degenerate text- ∅ pair in text classiﬁcation\nor sequence tagging."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nCompared to pre-training, ﬁne-tuning is rela-\ntively inexpensive. The only new parameters introduced during\nﬁne-tuning are classiﬁcation layer weights W ∈\nRK×H, where Kis the number of labels."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nAll of the results in the pa-\nper can be replicated in at most 1 hour on a sin-\ngle Cloud TPU, or a few hours on a GPU, starting\nfrom the exact same pre-trained model. 7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nDetailed descriptions of GLUE datasets are\nincluded in Appendix B.1. 8See (10) in https://gluebenchmark.com/faq."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard). The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERTLARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nThe number below each task denotes the number of training examples. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\n8 BERT and OpenAI GPT are single-\nmodel, single task. Note that\nBERTBASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nF1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. Table 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11 and are allowed to\nuse any public data when training their systems. Our best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. (2018), but the system\nhas improved substantially after publication."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nWe exclude entries that use BERT as one of their components. Given a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERTBASE and BERTLARGE ."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. We only introduce a start vec-\ntor S ∈RH and an end vector E ∈RH during\nﬁne-tuning. We ﬁne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32. We therefore use modest data augmentation in\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\net al., 2017) befor ﬁne-tuning on SQuAD."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nAdditionally, for BERTLARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. Both\nBERTBASE and BERTLARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. We ﬁnd that BERT LARGE signiﬁcantly outper-\nforms BERTBASE across all tasks, especially those\nwith very little training data. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nThe effect of model\nsize is explored more thoroughly in Section 5.2."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\n4.2 SQuAD v1.1\nThe Stanford Question Answering Dataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016). 10https://gluebenchmark.com/leaderboard\nWikipedia containing the answer, the task is to\npredict the answer text span in the passage. As shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the A embedding and the passage using\nthe B embedding. The probability of word i being the\nstart of the answer span is computed as a dot prod-\nuct between Ti and S followed by a softmax over\nall of the words in the paragraph: Pi = eS·Ti\n∑\nj eS·Tj ."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Bert paper\n\nThe analogous formula is used for the end of the\nanswer span. The score of a candidate span from\nposition ito position jis deﬁned as S·Ti + E·Tj,\nand the maximum scoring span where j ≥ i is\nused as a prediction. Without TriviaQA ﬁne-\n11QANet is described in Yu et al."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman - - 82.3 91.2\n#1 Ensemble - nlnet - - 86.0 91.7\n#2 Ensemble - QANet - - 84.5 90.5\nPublished\nBiDAF+ELMo (Single) - 85.6 - 85.8\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\nOurs\nBERTBASE (Single) 80.8 88.5 - -\nBERTLARGE (Single) 84.1 90.9 - -\nBERTLARGE (Ensemble) 85.8 91.8 - -\nBERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2: SQuAD 1.1 results. System Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman 86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n#2 Single - nlnet - - 74.2 77.1\nPublished\nunet (Ensemble) - - 71.4 74.9\nSLQA+ (Single) - 71.4 74.4\nOurs\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. The results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nThe BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds. We exclude entries that\nuse BERT as one of their components. 5 Ablation Studies\nIn this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12\n4.3 SQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic. We use a simple approach to extend the SQuAD\nv1.1 BERT model for this task."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nWe treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. 4.4 SWAG\nThe Situations With Adversarial Generations\n(SW AG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices. When ﬁne-tuning on the SW AG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). Re-\nsults are presented in Table 4."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nThe probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull =\nS·C+ E·C to the score of the best non-null span\n12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\nthat contain at least one of the provided possible answers. We did not use TriviaQA data for this model. The\nonly task-speciﬁc parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation C denotes a score for each choice\nwhich is normalized with a softmax layer."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nSystem Dev Test\nESIM+GloVe 51.9 52.7\nESIM+ELMo 59.1 59.2\nOpenAI GPT - 78.0\nBERTBASE 81.6 -\nBERTLARGE 86.6 86.3\nHuman (expert)† - 85.0\nHuman (5 annotations)† - 88.0\nTable 4: SW AG Dev and Test accuracies.†Human per-\nformance is measured with 100 samples, as reported in\nthe SW AG paper. BERT LARGE out-\nperforms the authors’ baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nˆsi,j = maxj≥iS·Ti + E·Tj."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nWe predict a non-null\nanswer when ˆsi,j > snull + τ, where the thresh-\nold τ is selected on the dev set to maximize F1. We observe a +5.1 F1 improvement over\nthe previous best system."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nWe\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48. We ﬁne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Bert paper\n\nAdditional"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nDev Set\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\n(Acc) (Acc) (Acc) (Acc) (F1)\nBERTBASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\nTable 5: Ablation over the pre-training tasks using the\nBERTBASE architecture. ablation studies can be found in Appendix C.\n5.1 Effect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERTBASE :\nNo NSP: A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task. In Table 5, we show that removing NSP\nhurts performance signiﬁcantly on QNLI, MNLI,\nand SQuAD 1.1."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\n“No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. LTR & No NSP: A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at ﬁne-tuning, because removing\nit introduced a pre-train/ﬁne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our ﬁne-tuning scheme."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\n“+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nWe ﬁrst examine the impact brought by the NSP\ntask. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing “No NSP” to “LTR & No NSP”. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nThe LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. This does\nsigniﬁcantly improve results on SQuAD, but the\nresults are still far worse than those of the pre-\ntrained bidirectional models. We recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nThe BiLSTM hurts\nperformance on the GLUE tasks. Results on selected GLUE tasks are shown in\nTable 6."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\n5.2 Effect of Model Size\nIn this section, we explore the effect of model size\non ﬁne-tuning task accuracy. In this table, we report the average Dev\nSet accuracy from 5 random restarts of ﬁne-tuning."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nWe trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERT BASE\ncontains 110M parameters and BERT LARGE con-\ntains 340M parameters."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signiﬁcant\nimprovements on top of models which are al-\nready quite large relative to the existing literature. For example, the largest Transformer explored in\nVaswani et al. It has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6. However, we believe that\nthis is the ﬁrst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been sufﬁ-\nciently pre-trained."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Bert paper\n\nPeters et al. (2018b) presented"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. Both of these prior works used a feature-\nbased approach — we hypothesize that when the\nmodel is ﬁne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeciﬁc models can beneﬁt from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small. However, the feature-based approach,\nwhere ﬁxed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speciﬁc model architecture to be added."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\n(2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\n5.3 Feature-based Approach with BERT\nAll of the BERT results presented so far have used\nthe ﬁne-tuning approach, where a simple classiﬁ-\ncation layer is added to the pre-trained model, and\nall parameters are jointly ﬁne-tuned on a down-\nstream task. In this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. System Dev F1 Test F1\nELMo (Peters et al., 2018a) 95.7 92.2\nCVT (Clark et al., 2018) - 92.6\nCSE (Akbik et al., 2018) - 93.1\nFine-tuning approach\nBERTLARGE 96.6 92.8\nBERTBASE 96.4 92.4\nFeature-based approach (BERTBASE )\nEmbeddings 91.0 -\nSecond-to-Last Hidden 95.6 -\nLast Hidden 94.9 -\nWeighted Sum Last Four Hidden 95.9 -\nConcat Last Four Hidden 96.1 -\nWeighted Sum All 12 Layers 95.5 -\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. To ablate the ﬁne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without ﬁne-tuning\nany parameters of BERT. This\ndemonstrates that BERT is effective for both ﬁne-\ntuning and feature-based approaches."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nSecond, there are major computational beneﬁts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nFollowing standard practice, we for-\nmulate this as a tagging task but do not use a CRF\nHyperparams Dev Set Accuracy\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\n3 768 12 5.84 77.9 79.8 88.4\n6 768 3 5.24 80.6 82.2 90.7\n6 768 12 4.68 81.9 84.8 91.3\n12 768 12 3.99 84.4 86.7 92.9\n12 1024 16 3.54 85.7 86.9 93.3\n24 1024 16 3.23 86.6 87.8 93.7\nTable 6: Ablation over BERT model size. Results are presented in Table 7."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\n#L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. “LM (ppl)” is the masked LM perplexity\nof held-out training data. layer in the output. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classiﬁcation layer."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nHyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nWe use the representation of\nthe ﬁrst sub-token as the input to the token-level\nclassiﬁer over the NER label set. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind ﬁne-tuning the entire model."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\nBERT LARGE\nperforms competitively with state-of-the-art meth-\nods."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Bert paper\n\n6 Conclusion\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to beneﬁt from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these ﬁndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Bert paper\n\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. Rie Kubota Ando and Tong Zhang. Luisa Bentivogli, Bernardo Magnini, Ido Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. John Blitzer, Ryan McDonald, and Fernando Pereira. Samuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. Peter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. Christopher Clark and Matt Gardner. Kevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. Ronan Collobert and Jason Weston. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. Andrew M Dai and Quoc V Le. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. William B Dolan and Chris Brockett. William Fedus, Ian Goodfellow, and Andrew M Dai. Dan Hendrycks and Kevin Gimpel. Felix Hill, Kyunghyun Cho, and Anna Korhonen. Jeremy Howard and Sebastian Ruder. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. Yacine Jernite, Samuel R. Bowman, and David Son-\ntag."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Bert paper\n\n2018. 2018. 2005. 2009. 2006. 2015. 1992. 2017. 2013. 2018. 2018. 2018. 2008. 2017. 2015. 2009. 2005. 2018. 2016. 2016. 2018. 2018. 2017."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Bert paper\n\nContextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1638–1649. Character-level lan-\nguage modeling with deeper self-attention. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817–1853. The ﬁfth PASCAL recognizing textual entailment\nchallenge. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120–128. Association for Computa-\ntional Linguistics. A large anno-\ntated corpus for learning natural language inference. Association for Computational Linguis-\ntics. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017) , pages 1–14, Vancou-\nver, Canada. Association for Computational Lin-\nguistics. One billion word benchmark for measur-\ning progress in statistical language modeling. Simple\nand effective multi-paragraph reading comprehen-\nsion. Semi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1914–\n1925. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160–167. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087. ImageNet: A Large-Scale Hierarchical\nImage Database. Automati-\ncally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop\non Paraphrasing (IWP2005). Maskgan: Better text generation via ﬁlling in\nthe . Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. Association for Computational Linguistics. Reinforced\nmnemonic reader for machine reading comprehen-\nsion. Discourse-based objectives for fast un-\nsupervised sentence representation learning."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Bert paper\n\narXiv\npreprint arXiv:1808.04444. In TAC. NIST. In EMNLP. arXiv\npreprint arXiv:1312.3005. In ACL. ACM. In CVPR09. arXiv preprint arXiv:1801.07736. CoRR, abs/1606.08415. In\nACL. In IJCAI. CoRR,\nabs/1705.00557."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Bert paper\n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Quoc Le and Tomas Mikolov. Hector J Levesque, Ernest Davis, and Leora Morgen-\nstern. Lajanugen Logeswaran and Honglak Lee. Bryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. Oren Melamud, Jacob Goldberger, and Ido Dagan. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. Curran Associates,\nInc.\nAndriy Mnih and Geoffrey E Hinton. In\nD. Koller, D. Schuurmans, Y . Curran As-\nsociates, Inc.\nAnkur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. Matthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Matthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. Richard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. Wilson L Taylor. Erik F Tjong Kim Sang and Fien De Meulder. Joseph Turian, Lev Ratinov, and Yoshua Bengio. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Alex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Bert paper\n\n2017. 2015. 2014. 2011. 2018. 2017. 2013. 2009. 2016. 2014. 2017. 2018a. 2018b. 2018. 2016. 2017. 2013. 2018. 1953. 2003. 2010. 2017. 2008. 2018a."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Bert paper\n\nTriviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning , pages\n1188–1196. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47. An\nefﬁcient framework for learning sentence represen-\ntations. In International Conference on Learning\nRepresentations. Learned in translation: Con-\ntextualized word vectors. In NIPS. 2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26 , pages 3111–3119. A scal-\nable hierarchical distributed language model. Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21 , pages 1081–1088. A decomposable attention\nmodel for natural language inference. In EMNLP. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1532–\n1543. Semi-supervised se-\nquence tagging with bidirectional language models. In ACL. Deep contextualized word rep-\nresentations. In NAACL. Dissecting contextual\nword embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499–1509. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392. Bidirectional attention\nﬂow for machine comprehension. In ICLR. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642. U-net: Machine reading comprehension\nwith unanswerable questions. arXiv preprint\narXiv:1810.06638. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL. Word representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL ’10, pages 384–394. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international\nconference on Machine learning, pages 1096–1103. ACM. Glue: A multi-task benchmark and analysis platform"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 11,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics. Neural network acceptability judg-\nments. A broad-coverage challenge corpus\nfor sentence understanding through inference. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320–3328. QANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19–27. Appendix for “BERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding”\nWe organize the appendix into three sections:\n• Additional implementation details for BERT\nare presented in Appendix A;\n• Additional details for our experiments are\npresented in Appendix B; and\n• Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n– Effect of Number of Training Steps; and\n– Ablation for Different Masking Proce-\ndures. A Additional Details for BERT\nA.1 Illustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following. Masked LM and the Masking ProcedureAs-\nsuming the unlabeled sentence is my dog is\nhairy, and during the random masking procedure\nwe chose the 4-th token (which corresponding to\nhairy), our masking procedure can be further il-\nlustrated by\n• 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy →\nmy dog is [MASK]\n• 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy → my\ndog is apple\n• 10% of the time: Keep the word un-\nchanged, e.g., my dog is hairy → my dog\nis hairy. The purpose of this is to bias the\nrepresentation towards the actual observed\nword. The advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\nery input token. Additionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model’s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure. Compared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 11,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\nWei Wang, Ming Yan, and Chen Wu. Alex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. Adina Williams, Nikita Nangia, and Samuel R Bow-\nman. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 11,
            "page_label": "1"
        },
        "page_content": "Bert paper\n\n2018b. 2018. arXiv preprint arXiv:1805.12471. 2018. In\nNAACL. 2016. arXiv preprint\narXiv:1609.08144. 2014. 2018. In ICLR. 2018. 2015."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nBERT (Ours)\nTrm Trm Trm\nTrm Trm Trm\n...\n...\nTrm Trm Trm\nTrm Trm Trm\n...\n...\nOpenAI GPT\nLstm\nELMo\nLstm Lstm\nLstm Lstm Lstm\nLstm Lstm Lstm\nLstm Lstm Lstm\n T1 T2  TN...\n...\n...\n...\n...\n E1 E2  EN...\n T1 T2 TN...\n E1 E2  EN...\n T1 T2  TN...\n E1 E2  EN... BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach. Training of BERT BASE was performed on 4\nCloud TPUs in Pod conﬁguration (16 TPU chips\ntotal).13 Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nFigure 3: Differences in pre-training model architectures. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost. Each pre-\ntraining took 4 days to complete. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. A.3 Fine-tuning Procedure\nFor ﬁne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nto converge."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nNext Sentence Prediction The next sentence\nprediction task can be illustrated in the following\nexamples. Input = [CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nInput = [CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\nA.2 Pre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as “sentences” even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The ﬁrst sentence receives the A\nembedding and the second receives the B embed-\nding. 50% of the time B is the actual next sentence\nthat follows A and 50% of the time it is a random\nsentence, which is done for the “next sentence pre-\ndiction” task. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nThey are sampled such that the com-\nbined length is ≤512 tokens. We train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40\nepochs over the 3.3 billion word corpus. We\nuse Adam with learning rate of 1e-4, β1 = 0.9,\nβ2 = 0.999, L2 weight decay of 0.01, learning\nrate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate. Longer sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 12,
            "page_label": "2"
        },
        "page_content": "Bert paper\n\nWe use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu, following OpenAI GPT. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speciﬁc, but we found the following range\nof possible values to work well across all tasks:\n• Batch size: 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 13,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\n• Learning rate (Adam): 5e-5, 3e-5, 2e-5\n• Number of epochs: 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. A.4 Comparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach. The most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords). • GPT uses a sentence separator ( [SEP]) and\nclassiﬁer token ( [CLS]) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP], [CLS] and sentence A/B embed-\ndings during pre-training. • GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words. • GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set. A.5 Illustrations of Fine-tuning on Different\nTasks\nThe illustration of ﬁne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speciﬁc\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe ﬁgure, E represents the input embedding, Ti\nrepresents the contextual representation of tokeni,\n[CLS] is the special symbol for classiﬁcation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 13,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\nFine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable. The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 13,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\nB Detailed Experimental Setup\nB.1 Detailed Descriptions for the GLUE\nBenchmark Experiments. Our GLUE results in Table1 are obtained\nfrom https://gluebenchmark.com/\nleaderboard and https://blog. The GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 13,
            "page_label": "3"
        },
        "page_content": "Bert paper\n\nopenai.com/language-unsupervised. (2018a):\nMNLI Multi-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classiﬁ-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the ﬁrst one. QQP Quora Question Pairs is a binary classiﬁ-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018). QNLI Question Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classiﬁcation task (Wang\net al., 2018a)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 14,
            "page_label": "4"
        },
        "page_content": "Bert paper\n\nBERT\nE[CLS] E1  E[SEP]... EN E1’ ... EM’\nC\n T1\n T[SEP]...\n TN\n T1’ ...\n TM’\n[CLS] Tok \n1\n [SEP]... Tok \nN\nTok \n1 ... Tok\nM\nQuestion Paragraph\nBERT\nE[CLS] E1  E2  EN\nC\n T1\n  T2\n  TN\nSingle Sentence \n...\n...\nBERT\nTok 1  Tok 2  Tok N...[CLS]\nE[CLS] E1  E2  EN\nC\n T1\n  T2\n  TN\nSingle Sentence \nB-PERO O\n...\n...E[CLS] E1  E[SEP]\nClass \nLabel\n... EN E1’ ... EM’\nC\n T1\n T[SEP]...\n TN\n T1’ ...\n TM’\nStart/End Span\nClass \nLabel\nBERT\nTok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok \n1\n [SEP]... Tok \nN\nTok \n1 ... Tok\nM\nSentence 1\n... SST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classiﬁcation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013). CoLA The Corpus of Linguistic Acceptability is\na binary single-sentence classiﬁcation task, where\nthe goal is to predict whether an English sentence\nis linguistically “acceptable” or not (Warstadt\net al., 2018). STS-B The Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005). RTE Recognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\nWNLI Winograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 14,
            "page_label": "4"
        },
        "page_content": "Bert paper\n\nSentence 2\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks. The GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that’s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n14Note that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI. 15https://gluebenchmark.com/faq"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\njority class."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nfor ksteps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nThis allows us to answer the following\nquestions:\n1. 2."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nQuestion: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy? Answer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps. 200 400 600 800 1,000\n76\n78\n80\n82\n84\nPre-training Steps (Thousands)\nMNLI Dev Accuracy\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. The numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nQuestion: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word? Answer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The x-axis is the value of k.\nNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; R ND means that\nwe replace the target token with another random\ntoken. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3. However, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nThe following is an ablation\nstudy to evaluate the effect of different masking\nstrategies. Masking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies. From the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nWe report the\nDev results for both MNLI and NER. For NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nThe results are presented in Table 8. The right part of the paper represents the\nDev set results."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.17",
            "creator": "LaTeX with hyperref package",
            "creationdate": "2019-05-28T00:07:51+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2019-05-28T00:07:51+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2",
            "subject": "",
            "title": "Bert paper",
            "trapped": "/False",
            "source": "./data/pdfs/Bert paper.pdf",
            "total_pages": 16,
            "page": 15,
            "page_label": "5"
        },
        "page_content": "Bert paper\n\nInterestingly, using only\nthe R ND strategy performs much worse than our\nstrategy as well."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nExperiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nWe show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\n∗Equal contribution. †Work performed while at Google Brain. ‡Work performed while at Google Research."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nListing order is random."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nJakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nAshish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\nNiki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 0,
            "page_label": "1"
        },
        "page_content": "Attention is all you need\n\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "Attention is all you need\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 1,
            "page_label": "2"
        },
        "page_content": "Attention is all you need\n\n2"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "Attention is all you need\n\nFigure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively. 3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 2,
            "page_label": "3"
        },
        "page_content": "Attention is all you need\n\nWe also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. 3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2)."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\n(right) Multi-Head Attention consists of several\nattention layers running in parallel."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key. We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nThe input consists of\nqueries and keys of dimension dk, and values of dimension dv. The keys and values are also packed together into matrices K and V ."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. 3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nWe compute\nthe matrix of outputs as:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nDot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk\n. To counteract this effect, we scale the dot products by 1√dk\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nWhile the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 3,
            "page_label": "4"
        },
        "page_content": "Attention is all you need\n\n4"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2. See Figure 2."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nWhere the projections are parameter matricesWQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv\nand WO ∈ Rhdv×dmodel . In this work we employ h = 8 parallel attention layers, or heads. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nFor each of these we use\ndk = dv = dmodel/h = 64. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. 3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nThis mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9]. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. 5"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nWe implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. 3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nThis\nconsists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 4,
            "page_label": "5"
        },
        "page_content": "Attention is all you need\n\nIn\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. One is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nn is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. 4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nTo this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9]. We also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E))."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nThe wavelengths form a geometric progression from 2π to 10000 · 2π."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nWe\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos. We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 5,
            "page_label": "6"
        },
        "page_content": "Attention is all you need\n\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Attention is all you need\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. 5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens. 5.4 Regularization\nWe employ three types of regularization during training:\n7"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Attention is all you need\n\nTo improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). A single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Attention is all you need\n\nWe plan to investigate this approach further in future work. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 6,
            "page_label": "7"
        },
        "page_content": "Attention is all you need\n\n5 Training\nThis section describes the training regime for our models. 5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days). 5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Attention is all you need\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model\nBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3. Table 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. 6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Attention is all you need\n\nFor the base model, we use a rate of\nPdrop = 0.1. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38]. 8"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Attention is all you need\n\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. Training took 3.5 days on 8 P100 GPUs. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 7,
            "page_label": "8"
        },
        "page_content": "Attention is all you need\n\nThe configuration of this model is\nlisted in the bottom line of Table 3. These hyperparameters\nwere chosen after experimentation on the development set."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nTable 3: Variations on the Transformer architecture. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nUnlisted values are identical to those of the base\nmodel. N d model dff h d k dv Pdrop ϵls\ntrain PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nAll metrics are on the English-to-German translation development set, newstest2013. We performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nListed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nWe used beam search as described in the previous section, but no\ncheckpoint averaging."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nThis\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. We trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. During inference, we\n9"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 8,
            "page_label": "9"
        },
        "page_content": "Attention is all you need\n\nWe also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences. In the former task our best\nmodel outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. Long short-term memory-networks for machine\nreading."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\n(2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\n(2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting. The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor. Layer normalization."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\n(2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. Making generation less sequential is another research goals of ours. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\nOn both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. Neural machine translation by jointly\nlearning to align and translate. Massive exploration of neural\nmachine translation architectures."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . [4] Jianpeng Cheng, Li Dong, and Mirella Lapata."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\narXiv preprint\narXiv:1607.06450, 2016. CoRR, abs/1409.0473, 2014. CoRR, abs/1703.03906, 2017. arXiv preprint arXiv:1601.06733, 2016."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 9,
            "page_label": "10"
        },
        "page_content": "Attention is all you need\n\nLe. 10"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Attention is all you need\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. [6] Francois Chollet. [7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. [10] Alex Graves. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. [13] Sepp Hochreiter and Jürgen Schmidhuber. [14] Zhongqiang Huang and Mary Harper. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. [16] Łukasz Kaiser and Samy Bengio. [17] Łukasz Kaiser and Ilya Sutskever. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. [20] Diederik Kingma and Jimmy Ba. [21] Oleksii Kuchaiev and Boris Ginsburg. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. [23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Attention is all you need\n\nLearning phrase representations using rnn encoder-decoder for statistical\nmachine translation. Xception: Deep learning with depthwise separable convolutions. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. Recurrent neural\nnetwork grammars. Convolu-\ntional sequence to sequence learning. Generating sequences with recurrent neural networks. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. Exploring\nthe limits of language modeling. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n2017. Structured attention networks. In International Conference on Learning Representations, 2017. Adam: A method for stochastic optimization. Factorization tricks for LSTM networks. A structured self-attentive sentence embedding. Multi-task\nsequence to sequence learning. Effective approaches to attention-\nbased neural machine translation."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 10,
            "page_label": "11"
        },
        "page_content": "Attention is all you need\n\nCoRR, abs/1406.1078, 2014. arXiv\npreprint arXiv:1610.02357, 2016. CoRR, abs/1412.3555, 2014. In Proc. of NAACL, 2016. arXiv preprint arXiv:1705.03122v2, 2017. arXiv preprint\narXiv:1308.0850, 2013. ACL, August 2009. arXiv preprint arXiv:1602.02410, 2016. In ICLR, 2015. arXiv preprint\narXiv:1703.10722, 2017. arXiv preprint\narXiv:1703.03130, 2017. arXiv preprint arXiv:1511.06114, 2015. arXiv preprint arXiv:1508.04025, 2015. 11"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "Attention is all you need\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. [26] David McClosky, Eugene Charniak, and Mark Johnson. [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. [28] Romain Paulus, Caiming Xiong, and Richard Socher. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "Attention is all you need\n\nBuilding a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016. A deep reinforced model for abstractive\nsummarization. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. Using the output embedding to improve language models. Neural machine translation of rare words\nwith subword units. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014. Rethinking the inception architecture for computer vision. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. Deep recurrent models with\nfast-forward connections for neural machine translation. Fast and accurate\nshift-reduce constituent parsing."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "Attention is all you need\n\nACL, June 2006. ACL, July\n2006. Curran Associates,\nInc., 2015. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 11,
            "page_label": "12"
        },
        "page_content": "Attention is all you need\n\narXiv preprint arXiv:1705.04304, 2017. [30] Ofir Press and Lior Wolf. arXiv\npreprint arXiv:1608.05859, 2016. arXiv preprint arXiv:1508.07909, 2015. arXiv preprint arXiv:1701.06538, 2017. CoRR, abs/1512.00567, 2015. arXiv preprint\narXiv:1609.08144, 2016. CoRR, abs/1606.04199, 2016. 12"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\nAttention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n. <EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\nMany of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\nDifferent colors represent different heads."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\nBest viewed in color."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 12,
            "page_label": "13"
        },
        "page_content": "Attention is all you need\n\n13"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 13,
            "page_label": "14"
        },
        "page_content": "Attention is all you need\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n. <EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 13,
            "page_label": "14"
        },
        "page_content": "Attention is all you need\n\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n. <EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 13,
            "page_label": "14"
        },
        "page_content": "Attention is all you need\n\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 13,
            "page_label": "14"
        },
        "page_content": "Attention is all you need\n\n14"
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 14,
            "page_label": "15"
        },
        "page_content": "Attention is all you need\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n. <EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 14,
            "page_label": "15"
        },
        "page_content": "Attention is all you need\n\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n. <EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 14,
            "page_label": "15"
        },
        "page_content": "Attention is all you need\n\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks."
    },
    {
        "metadata": {
            "producer": "pdfTeX-1.40.25",
            "creator": "LaTeX with hyperref",
            "creationdate": "2024-04-10T21:11:43+00:00",
            "author": "",
            "keywords": "",
            "moddate": "2024-04-10T21:11:43+00:00",
            "ptex.fullbanner": "This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5",
            "subject": "",
            "title": "Attention is all you need",
            "trapped": "/False",
            "source": "./data/pdfs/Attention is all you need.pdf",
            "total_pages": 15,
            "page": 14,
            "page_label": "15"
        },
        "page_content": "Attention is all you need\n\n15"
    }
]