"""
Evaluation metrics module for the RAG system.

This module contains various metrics for evaluating the performance of
different chunking methods in the RAG system.
"""

import numpy as np
from typing import List, Dict, Any, Optional, Union
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from langchain.schema import Document


def calculate_bleu(generated_answer: str, reference_answer: str) -> float:
    """
    Computes the BLEU score between the generated and reference answers.
    
    BLEU (Bilingual Evaluation Understudy) is a precision-based metric that measures
    the n-gram overlap between the generated text and reference text. It was originally
    designed for machine translation evaluation but is also used for text generation tasks.
    
    The score ranges from 0 to 1, where:
    - 0 indicates no overlap
    - 1 indicates perfect overlap (identical texts)
    
    This implementation uses smoothing to handle cases where there are no n-gram matches.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        
    Returns:
        float: The BLEU score.
    """
    smoothing_function = SmoothingFunction().method1
    return sentence_bleu([reference_answer.split()], generated_answer.split(), smoothing_function=smoothing_function)


def calculate_rouge(generated_answer: str, reference_answer: str) -> Dict[str, float]:
    """
    Computes ROUGE scores between the generated and reference answers.
    
    ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap
    of n-grams between the generated and reference texts:
    - ROUGE-1: Overlap of unigrams (single words)
    - ROUGE-2: Overlap of bigrams (word pairs)
    - ROUGE-L: Longest Common Subsequence, measuring the longest matching sequence
    
    Higher scores indicate better overlap between generated and reference answers.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        
    Returns:
        Dict[str, float]: Dictionary containing ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.
    """
    rouge = Rouge()
    scores = rouge.get_scores(generated_answer, reference_answer)[0]
    return {
        'rouge-1': scores['rouge-1']['f'],
        'rouge-2': scores['rouge-2']['f'],
        'rouge-l': scores['rouge-l']['f']
    }


def calculate_exact_match(generated_answer: str, reference_answer: str) -> int:
    """
    Computes the exact match score between the generated and reference answers.
    
    Exact Match (EM) is a binary metric that returns 1 if the generated answer
    exactly matches the reference answer after normalization (lowercasing and
    stripping whitespace), and 0 otherwise.
    
    This is a strict metric that requires perfect matching and doesn't account
    for partial correctness or semantic similarity.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        
    Returns:
        int: 1 if the answers match exactly, 0 otherwise.
    """
    return int(generated_answer.strip().lower() == reference_answer.strip().lower())


def calculate_f1_score(generated_answer: str, reference_answer: str) -> float:
    """
    Computes the F1 score between the generated and reference answers.
    
    F1 score is the harmonic mean of precision and recall at the word level:
    - Precision: The fraction of words in the generated answer that are also in the reference answer
    - Recall: The fraction of words in the reference answer that are also in the generated answer
    - F1 = 2 * (precision * recall) / (precision + recall)
    
    This metric balances precision and recall, providing a measure of partial correctness
    that is more flexible than exact match. The score ranges from 0 to 1, where higher
    values indicate better performance.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        
    Returns:
        float: The F1 score.
    """
    gen_tokens = set(generated_answer.lower().split())
    ref_tokens = set(reference_answer.lower().split())
    
    common = gen_tokens & ref_tokens
    if not common:
        return 0.0
    precision = len(common) / len(gen_tokens)
    recall = len(common) / len(ref_tokens)
    return 2 * precision * recall / (precision + recall)


def calculate_semantic_similarity(
    generated_answer: str, 
    reference_answer: str,
    model: Optional[SentenceTransformer] = None
) -> float:
    """
    Computes the cosine similarity between sentence embeddings of the generated and reference answers.
    
    Semantic similarity uses sentence embeddings to capture the meaning of texts beyond
    simple word overlap. This method:
    1. Encodes both the generated and reference answers into dense vector representations
    2. Computes the cosine similarity between these vectors
    
    The score ranges from -1 to 1, where:
    - 1 indicates perfect semantic similarity (identical meaning)
    - 0 indicates no semantic relationship
    - -1 indicates opposite meanings (rarely occurs in practice)
    
    This metric is particularly valuable for evaluating RAG systems as it can detect
    when answers are semantically correct even if they use different wording.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        model (Optional[SentenceTransformer]): Pre-loaded sentence transformer model.
            If None, a new model will be loaded.
            
    Returns:
        float: The semantic similarity score.
    """
    if model is None:
        model = SentenceTransformer('BAAI/bge-small-en-v1.5')
    
    gen_embedding = model.encode([generated_answer], convert_to_tensor=True)
    ref_embedding = model.encode([reference_answer], convert_to_tensor=True)

    # Move tensors to CPU before converting to numpy arrays
    gen_embedding = gen_embedding.cpu().numpy()
    ref_embedding = ref_embedding.cpu().numpy()

    return cosine_similarity(gen_embedding, ref_embedding)[0][0]


def calculate_precision_at_k(
    relevant_docs: List[Document],
    retrieved_docs: List[Document],
    k: int
) -> float:
    """
    Computes the precision@k metric for document retrieval.
    
    Precision@k is the proportion of retrieved documents in the top-k
    that are relevant.
    
    Args:
        relevant_docs (List[Document]): List of relevant documents.
        retrieved_docs (List[Document]): List of retrieved documents.
        k (int): The cutoff rank.
        
    Returns:
        float: The precision@k score.
    """
    if not retrieved_docs or k <= 0:
        return 0.0
    
    # Get the top-k retrieved documents
    top_k_docs = retrieved_docs[:min(k, len(retrieved_docs))]
    
    # Count how many of the top-k are relevant
    relevant_in_top_k = 0
    relevant_ids = {doc.metadata.get('id', i) for i, doc in enumerate(relevant_docs)}
    
    for doc in top_k_docs:
        doc_id = doc.metadata.get('id', None)
        if doc_id in relevant_ids:
            relevant_in_top_k += 1
    
    return relevant_in_top_k / len(top_k_docs)


def calculate_recall_at_k(
    relevant_docs: List[Document],
    retrieved_docs: List[Document],
    k: int
) -> float:
    """
    Computes the recall@k metric for document retrieval.
    
    Recall@k is the proportion of relevant documents that are retrieved
    in the top-k.
    
    Args:
        relevant_docs (List[Document]): List of relevant documents.
        retrieved_docs (List[Document]): List of retrieved documents.
        k (int): The cutoff rank.
        
    Returns:
        float: The recall@k score.
    """
    if not relevant_docs or not retrieved_docs or k <= 0:
        return 0.0
    
    # Get the top-k retrieved documents
    top_k_docs = retrieved_docs[:min(k, len(retrieved_docs))]
    
    # Count how many of the relevant documents are in the top-k
    relevant_in_top_k = 0
    relevant_ids = {doc.metadata.get('id', i) for i, doc in enumerate(relevant_docs)}
    
    for doc in top_k_docs:
        doc_id = doc.metadata.get('id', None)
        if doc_id in relevant_ids:
            relevant_in_top_k += 1
    
    return relevant_in_top_k / len(relevant_docs)


def calculate_mrr(
    relevant_docs: List[Document],
    retrieved_docs: List[Document]
) -> float:
    """
    Computes the Mean Reciprocal Rank (MRR) for document retrieval.
    
    MRR is the average of the reciprocal ranks of the first relevant
    document for each query. The reciprocal rank is the multiplicative
    inverse of the rank of the first relevant document.
    
    Args:
        relevant_docs (List[Document]): List of relevant documents.
        retrieved_docs (List[Document]): List of retrieved documents.
        
    Returns:
        float: The MRR score.
    """
    if not relevant_docs or not retrieved_docs:
        return 0.0
    
    # Get the IDs of relevant documents
    relevant_ids = {doc.metadata.get('id', i) for i, doc in enumerate(relevant_docs)}
    
    # Find the rank of the first relevant document
    for i, doc in enumerate(retrieved_docs):
        doc_id = doc.metadata.get('id', None)
        if doc_id in relevant_ids:
            return 1.0 / (i + 1)  # Reciprocal rank
    
    return 0.0  # No relevant document found


def calculate_chunk_utilization(
    retrieved_docs: List[Document],
    used_chunks: List[Document]
) -> float:
    """
    Computes the chunk utilization metric.
    
    Chunk utilization measures what percentage of retrieved chunks
    were actually used in generating the answer.
    
    Args:
        retrieved_docs (List[Document]): List of all retrieved documents.
        used_chunks (List[Document]): List of chunks actually used in the answer.
        
    Returns:
        float: The chunk utilization score (0.0 to 1.0).
    """
    if not retrieved_docs:
        return 0.0
    
    if not used_chunks:
        return 0.0
    
    # Get the IDs of used chunks
    used_ids = {chunk.metadata.get('id', i) for i, chunk in enumerate(used_chunks)}
    
    # Count how many of the retrieved chunks were used
    used_count = 0
    for doc in retrieved_docs:
        doc_id = doc.metadata.get('id', None)
        if doc_id in used_ids:
            used_count += 1
    
    return used_count / len(retrieved_docs)


def calculate_efficiency_metrics(
    chunking_time: float,
    retrieval_time: float,
    generation_time: float,
    num_chunks: int,
    avg_chunk_size: int
) -> Dict[str, float]:
    """
    Computes efficiency metrics for the RAG pipeline.
    
    Args:
        chunking_time (float): Time taken for chunking (seconds).
        retrieval_time (float): Time taken for retrieval (seconds).
        generation_time (float): Time taken for answer generation (seconds).
        num_chunks (int): Number of chunks created.
        avg_chunk_size (int): Average chunk size (in characters or words).
        
    Returns:
        Dict[str, float]: Dictionary containing efficiency metrics.
    """
    total_time = chunking_time + retrieval_time + generation_time
    
    return {
        'chunking_time': chunking_time,
        'retrieval_time': retrieval_time,
        'generation_time': generation_time,
        'total_time': total_time,
        'num_chunks': num_chunks,
        'avg_chunk_size': avg_chunk_size,
        'chunks_per_second': num_chunks / chunking_time if chunking_time > 0 else 0,
        'retrieval_latency': retrieval_time,
        'generation_latency': generation_time
    }


def calculate_all_metrics(
    generated_answer: str,
    reference_answer: str,
    relevant_docs: List[Document],
    retrieved_docs: List[Document],
    used_chunks: Optional[List[Document]] = None,
    efficiency_data: Optional[Dict[str, float]] = None,
    k_values: List[int] = [1, 3, 5, 10],
    model: Optional[SentenceTransformer] = None
) -> Dict[str, Union[float, Dict[str, float]]]:
    """
    Computes all evaluation metrics for a single query.
    
    Args:
        generated_answer (str): The answer generated by the system.
        reference_answer (str): The reference (ground truth) answer.
        relevant_docs (List[Document]): List of relevant documents.
        retrieved_docs (List[Document]): List of retrieved documents.
        used_chunks (Optional[List[Document]]): List of chunks actually used in the answer.
        efficiency_data (Optional[Dict[str, float]]): Efficiency metrics data.
        k_values (List[int]): List of k values for precision@k and recall@k.
        model (Optional[SentenceTransformer]): Pre-loaded sentence transformer model.
        
    Returns:
        Dict[str, Union[float, Dict[str, float]]]: Dictionary containing all metrics.
    """
    metrics = {}
    
    # Text generation metrics
    metrics['bleu'] = calculate_bleu(generated_answer, reference_answer)
    metrics['exact_match'] = calculate_exact_match(generated_answer, reference_answer)
    metrics['f1_score'] = calculate_f1_score(generated_answer, reference_answer)
    metrics['semantic_similarity'] = calculate_semantic_similarity(
        generated_answer, reference_answer, model
    )
    
    # ROUGE metrics
    rouge_scores = calculate_rouge(generated_answer, reference_answer)
    metrics.update(rouge_scores)
    
    # Retrieval metrics
    metrics['mrr'] = calculate_mrr(relevant_docs, retrieved_docs)
    
    # Precision@k and Recall@k for different k values
    for k in k_values:
        metrics[f'precision@{k}'] = calculate_precision_at_k(relevant_docs, retrieved_docs, k)
        metrics[f'recall@{k}'] = calculate_recall_at_k(relevant_docs, retrieved_docs, k)
    
    # Chunk utilization if data is available
    if used_chunks is not None:
        metrics['chunk_utilization'] = calculate_chunk_utilization(retrieved_docs, used_chunks)
    
    # Efficiency metrics if data is available
    if efficiency_data is not None:
        metrics['efficiency'] = calculate_efficiency_metrics(
            efficiency_data.get('chunking_time', 0),
            efficiency_data.get('retrieval_time', 0),
            efficiency_data.get('generation_time', 0),
            efficiency_data.get('num_chunks', 0),
            efficiency_data.get('avg_chunk_size', 0)
        )
    
    return metrics